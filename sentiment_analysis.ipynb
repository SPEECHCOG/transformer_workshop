{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "Tutorial from [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/) by Prodip Hore and Sayan Chatterjee\n",
    "\n",
    "## Dataset\n",
    "\n",
    "UCI Machine Learning Repository: Sentiment Labelled Sentences Data Set\n",
    "('From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015)\n",
    "\n",
    "Sentences: 3000\n",
    "Labels: Positive (1) - Negative (0)\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "* \"The mic is great.\" Positive ->  `The mic is great.\t1`\n",
    "\n",
    "* \"What a waste of money and time!.\" Negative -> `What a waste of money and time!.\t0`\n",
    "\n",
    "\n",
    "## Architecture\n",
    "\n",
    "Input layer -> Embedding layer -> LSTM -> Dense (softmax) -> Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "with open('data/amazon.txt', mode='r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "with open('data/yelp.txt', mode='r') as f:\n",
    "    lines += f.readlines()\n",
    "\n",
    "sentences = [line.split('\\t')[0] for line in lines]\n",
    "labels = [int(line.split('\\t')[1]) for line in lines]\n",
    "labels = np.asarray(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentences)\n",
    "text_matrix= t.texts_to_sequences(sentences)\n",
    "\n",
    "len_mat = []\n",
    "for i in range(len(text_matrix)):\n",
    "    len_mat.append(len(text_matrix[i]))\n",
    "\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "1600\n",
      "400\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features = 32\n",
    "\n",
    "tex_pad = pad_sequences(text_matrix, maxlen=features, padding='post')\n",
    "\n",
    "x_train = tex_pad[:1600,:]\n",
    "y_train = labels[:1600]\n",
    "x_test = tex_pad[1600:,:]\n",
    "y_test = labels[1600:]\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 32, 32)            104288    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 157,589\n",
      "Trainable params: 157,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "inputs = Input(shape=(features,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=features, input_length=features, embeddings_regularizer=l2(.001))\n",
    "embd_out = embedding(inputs)\n",
    "lstm = LSTM(100, dropout=0.3, recurrent_dropout=0.2)\n",
    "lstm_out = lstm(embd_out)\n",
    "\n",
    "prob = Dense(1, activation='sigmoid')\n",
    "outputs = prob(lstm_out)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/workshop/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 3s 2ms/sample - loss: 0.7488 - acc: 0.5206 - val_loss: 0.7300 - val_acc: 0.4125\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 1s 832us/sample - loss: 0.7090 - acc: 0.5219 - val_loss: 0.7088 - val_acc: 0.4125\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 1s 767us/sample - loss: 0.6964 - acc: 0.5219 - val_loss: 0.7010 - val_acc: 0.4125\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 1s 931us/sample - loss: 0.6933 - acc: 0.5188 - val_loss: 0.7039 - val_acc: 0.4125\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 1s 766us/sample - loss: 0.6924 - acc: 0.5219 - val_loss: 0.6989 - val_acc: 0.4200\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 1s 770us/sample - loss: 0.6849 - acc: 0.5494 - val_loss: 0.6842 - val_acc: 0.5325\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 1s 804us/sample - loss: 0.5726 - acc: 0.7375 - val_loss: 0.5592 - val_acc: 0.7400\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 1s 736us/sample - loss: 0.3877 - acc: 0.8687 - val_loss: 0.5117 - val_acc: 0.7850\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 1s 929us/sample - loss: 0.2765 - acc: 0.9144 - val_loss: 0.5698 - val_acc: 0.7825\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 1s 845us/sample - loss: 0.1758 - acc: 0.9563 - val_loss: 0.7136 - val_acc: 0.7725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7d641dfa58>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "\n",
    "model.fit(x=x_train,y=y_train,\n",
    "          batch_size=100,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_test,y_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i miss it and wish they had one in philadelphia', 'we got sitting fairly fast but ended up waiting 40 minutes just to place our order another 30 minutes before the food arrived', 'they also have the best cheese crisp in town', 'good value great food great service', \"couldn't ask for a more satisfying meal\", 'the food is good', 'it was awesome', 'i just wanted to leave', 'we made the drive all the way from north scottsdale and i was not one bit disappointed', 'i will not be eating there again']\n",
      "[1 0 1 1 1 1 1 0 1 0]\n",
      "[[0.10233185]\n",
      " [0.06516251]\n",
      " [0.98587275]\n",
      " [0.9905857 ]\n",
      " [0.10396948]\n",
      " [0.988444  ]\n",
      " [0.98555195]\n",
      " [0.97717357]\n",
      " [0.03623567]\n",
      " [0.03208158]]\n"
     ]
    }
   ],
   "source": [
    "print(t.sequences_to_texts(x_test[:10]))\n",
    "print(y_test[:10])\n",
    "\n",
    "pred = model.predict(x_test[:10])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Input layer -> Embedding layer -> LSTM -> Attention -> Dense (softmax) -> Label\n",
    "\n",
    "### Attention (Bahdanau et al., 2015)\n",
    "Additive Attention\n",
    "\n",
    "1. $\\large score(s_t, h_i) = v_a^T \\text{tanh}(W_a[s_t;h_i])$ -> $\\large \\text{tanh}(W_ah + b)$\n",
    "\n",
    "2. $\\large \\alpha_{ti}=\\frac{exp(score_{ti})}{\\sum_{k=1}{N}{exp(score_{tk})}}$\n",
    "\n",
    "3. $\\large \\alpha \\cdot h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BahdanauAttention, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(BahdanauAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1), at\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        return super(BahdanauAttention, self).get_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 32, 32)            104288    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32, 100)           53200     \n",
      "_________________________________________________________________\n",
      "bahdanau_attention (Bahdanau ((None, 100), (None, 32,  132       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 157,721\n",
      "Trainable params: 157,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Attention, GlobalAveragePooling1D\n",
    "\n",
    "inputs1 = Input(shape=(features,))\n",
    "embedding1 = Embedding(input_dim=vocab_size, output_dim=features, input_length=features, embeddings_regularizer=l2(.001))\n",
    "embd_out1 = embedding1(inputs1)\n",
    "lstm1 = LSTM(100, dropout=0.3, recurrent_dropout=0.2, return_sequences=True)\n",
    "lstm_out1 = lstm1(embd_out1)\n",
    "\n",
    "# attention = GlobalAveragePooling1D(Attention()([lstm_out1, lstm_out1]))\n",
    "weighted_values, weights = BahdanauAttention()(lstm_out1)\n",
    "\n",
    "prob1 = Dense(1, activation='sigmoid')\n",
    "outputs1 = prob(weighted_values)\n",
    "\n",
    "model1 = Model(inputs1, outputs1)\n",
    "attention_model = Model(inputs1, weights)\n",
    "\n",
    "\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/workshop/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/andrea/miniconda3/envs/workshop/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 3s 2ms/sample - loss: 0.7494 - acc: 0.5206 - val_loss: 0.7329 - val_acc: 0.4125\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 1s 910us/sample - loss: 0.7088 - acc: 0.5219 - val_loss: 0.7080 - val_acc: 0.4200\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 1s 860us/sample - loss: 0.6662 - acc: 0.5900 - val_loss: 0.5732 - val_acc: 0.7100\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 1s 893us/sample - loss: 0.4623 - acc: 0.8037 - val_loss: 0.5428 - val_acc: 0.7350\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 2s 960us/sample - loss: 0.2974 - acc: 0.9013 - val_loss: 0.5688 - val_acc: 0.7675\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 1s 858us/sample - loss: 0.2046 - acc: 0.9356 - val_loss: 0.5113 - val_acc: 0.8125\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 2s 970us/sample - loss: 0.1450 - acc: 0.9581 - val_loss: 0.5997 - val_acc: 0.8050\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 1s 864us/sample - loss: 0.1085 - acc: 0.9750 - val_loss: 0.7631 - val_acc: 0.7650\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 2s 959us/sample - loss: 0.0965 - acc: 0.9787 - val_loss: 0.7458 - val_acc: 0.7800\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 1s 853us/sample - loss: 0.0683 - acc: 0.9894 - val_loss: 0.9187 - val_acc: 0.7700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7cfd08f080>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model1.fit(x=x_train,y=y_train,\n",
    "          batch_size=100,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_test,y_test)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i miss it and wish they had one in philadelphia', 'we got sitting fairly fast but ended up waiting 40 minutes just to place our order another 30 minutes before the food arrived', 'they also have the best cheese crisp in town', 'good value great food great service', \"couldn't ask for a more satisfying meal\", 'the food is good', 'it was awesome', 'i just wanted to leave', 'we made the drive all the way from north scottsdale and i was not one bit disappointed', 'i will not be eating there again']\n",
      "[[   3 2866    5    2 1101   37   25   40   14 2867    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [  32  108  819  756  331   28  661   52  425  727  124   50    6   26\n",
      "    78  198  209  592  124  205    1   24  364    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [  37   63   22    1   53  912  854   14  444    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [  17  396   18   24   18   29    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [ 236  469   12    4   89 1400  289    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   1   24    7   17    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   5    8  184    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   3   50  424    6  703    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [  32  107    1  715   33    1  141   47 2868 2869    2    3    8   11\n",
      "    40  306  114    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   3   56   11   31  390   58   90    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "[1 0 1 1 1 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(t.sequences_to_texts(x_test[:10]))\n",
    "print(x_test[:10])\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 1)\n",
      "[[31]\n",
      " [31]\n",
      " [20]\n",
      " [18]\n",
      " [31]\n",
      " [23]\n",
      " [24]\n",
      " [31]\n",
      " [31]\n",
      " [22]]\n",
      "[[0.9543078 ]\n",
      " [0.07817101]\n",
      " [0.9978306 ]\n",
      " [0.9991669 ]\n",
      " [0.01837063]\n",
      " [0.99657476]\n",
      " [0.99649674]\n",
      " [0.06141832]\n",
      " [0.04383131]\n",
      " [0.005243  ]]\n"
     ]
    }
   ],
   "source": [
    "pred = model1.predict(x_test[:10])\n",
    "attention_pred = attention_model.predict(x_test[:10])\n",
    "\n",
    "print(attention_pred.shape)\n",
    "print(np.argmax(attention_pred, axis=1))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.02454229 0.02461432 0.02473488 0.02483217 0.02492744 0.02502763\n",
      "   0.02515558 0.02524929 0.02535192 0.02551333 0.02570811 0.02591711\n",
      "   0.02617419 0.02654193 0.02705272 0.0276044  0.02817859 0.02858539\n",
      "   0.02883467 0.02894229 0.02918516 0.02973128 0.03076462 0.03247773\n",
      "   0.03503154 0.03831901 0.04183935 0.0448849  0.04706173 0.04841043\n",
      "   0.04918556 0.04962051]]\n",
      "\n",
      " [[0.02423973 0.02435366 0.02445743 0.0245304  0.02461175 0.0247054\n",
      "   0.02480593 0.02488237 0.02498384 0.02517285 0.02540021 0.02566215\n",
      "   0.02598485 0.02635182 0.02687965 0.02740537 0.02796045 0.02833679\n",
      "   0.02855904 0.02867477 0.02896723 0.02950063 0.03050787 0.03215694\n",
      "   0.03459605 0.03778385 0.04144003 0.04501927 0.048005   0.05015386\n",
      "   0.05153949 0.05237128]]\n",
      "\n",
      " [[0.01947381 0.01953141 0.01964355 0.01972592 0.01983152 0.01992946\n",
      "   0.02006838 0.02023746 0.02054298 0.02104525 0.0218302  0.02299102\n",
      "   0.02472147 0.02721487 0.03044251 0.03386971 0.03684956 0.03887039\n",
      "   0.0400018  0.04045329 0.04058119 0.04055452 0.04046807 0.04036547\n",
      "   0.04027901 0.04020389 0.04014126 0.04008885 0.04005143 0.04002033\n",
      "   0.03999559 0.03997576]]\n",
      "\n",
      " [[0.01785394 0.01792045 0.01806096 0.01824575 0.01856539 0.01914775\n",
      "   0.02000159 0.02131653 0.02327274 0.02590211 0.02888477 0.03161349\n",
      "   0.03371004 0.03519795 0.03627928 0.03705752 0.03764454 0.03797209\n",
      "   0.0380772  0.03796377 0.03778576 0.03759291 0.03741367 0.03725717\n",
      "   0.03713687 0.03703981 0.03696229 0.03689949 0.03685413 0.0368172\n",
      "   0.03678802 0.03676479]]\n",
      "\n",
      " [[0.02119894 0.02126196 0.02136216 0.02143196 0.02149828 0.02157489\n",
      "   0.02169241 0.02175908 0.02187626 0.02206217 0.02231338 0.02260546\n",
      "   0.02298679 0.02354062 0.02433921 0.02535458 0.02665551 0.02816916\n",
      "   0.02999807 0.032187   0.03487444 0.03783712 0.04065246 0.04293213\n",
      "   0.04454453 0.04555892 0.04615054 0.04647946 0.046664   0.04676474\n",
      "   0.04682095 0.0468529 ]]\n",
      "\n",
      " [[0.02025894 0.02030823 0.02038478 0.02041586 0.02051295 0.02060063\n",
      "   0.02071435 0.02082413 0.02100411 0.02129381 0.02172195 0.02231251\n",
      "   0.02318447 0.02452427 0.02653012 0.0292398  0.03255454 0.03584279\n",
      "   0.0384957  0.04016881 0.04109415 0.0415308  0.04170008 0.04173752\n",
      "   0.04173001 0.0417011  0.04166691 0.04163285 0.04160807 0.04158583\n",
      "   0.04156753 0.04155248]]\n",
      "\n",
      " [[0.0202699  0.02036453 0.02048906 0.02054936 0.02061461 0.02068503\n",
      "   0.02078767 0.02088912 0.02106122 0.02134114 0.0217548  0.02232257\n",
      "   0.02315866 0.02444388 0.02637339 0.02899572 0.03224909 0.03554724\n",
      "   0.03827951 0.04005115 0.04105541 0.04154264 0.04174093 0.04179428\n",
      "   0.04179553 0.04177157 0.0417403  0.04170807 0.04168449 0.04166307\n",
      "   0.04164536 0.04163074]]\n",
      "\n",
      " [[0.02372582 0.02381762 0.02395428 0.02405886 0.0241197  0.02417346\n",
      "   0.02425772 0.02432037 0.0244274  0.02459555 0.02481424 0.02504506\n",
      "   0.02532397 0.02571471 0.02625268 0.02684394 0.02747726 0.0279762\n",
      "   0.02836039 0.02865702 0.02915132 0.03002067 0.03144382 0.03356857\n",
      "   0.03645264 0.03988178 0.04338695 0.04643093 0.04871119 0.05021985\n",
      "   0.05113989 0.05167618]]\n",
      "\n",
      " [[0.0229304  0.02300571 0.0231471  0.0232058  0.02328344 0.02337855\n",
      "   0.02345383 0.0235167  0.02360634 0.02375912 0.02396688 0.02417048\n",
      "   0.02445061 0.02477925 0.02527306 0.02580141 0.02645242 0.02706484\n",
      "   0.02768331 0.02838731 0.02949825 0.03122052 0.03368728 0.03683389\n",
      "   0.04032182 0.04358479 0.04618017 0.04797664 0.04910571 0.04976794\n",
      "   0.05014574 0.05036061]]\n",
      "\n",
      " [[0.01870144 0.01878881 0.01887989 0.01896312 0.01905558 0.01918908\n",
      "   0.01936755 0.01957285 0.0199075  0.02042675 0.02119249 0.0222698\n",
      "   0.02381224 0.02599755 0.02888756 0.03219499 0.03546825 0.03809765\n",
      "   0.03988717 0.04088022 0.04138616 0.0416026  0.04166708 0.0416582\n",
      "   0.04162927 0.04159122 0.04155336 0.04151833 0.04149267 0.04147044\n",
      "   0.0414524  0.0414378 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(attention_pred.reshape(10,1,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
