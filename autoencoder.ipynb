{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From (https://blog.keras.io/building-autoencoders-in-keras.html) By [Francois Chollet](https://twitter.com/fchollet)\n",
    "# Autoencoders\n",
    "\n",
    "\n",
    "![autoencoder](https://www.compthree.com/images/blog/ae/ae.png)\n",
    "\n",
    "Usually, they are used for extracting latent representations with a lower dimensionality than the input data -> data compression \n",
    "\n",
    "## Case: MNIST handwritten digits\n",
    "\n",
    "### Dataset: \n",
    "\n",
    "Train images: 60,000\n",
    "\n",
    "Test images: 10,000\n",
    "\n",
    "Image size: 28x28 pixels\n",
    "\n",
    "![dataset](https://www.researchgate.net/profile/Steven_Young11/publication/306056875/figure/fig1/AS:393921575309346@1470929630835/Example-images-from-the-MNIST-dataset_W640.jpg)\n",
    "![image of a digit](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/05/Examples-from-the-MNIST-dataset.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST dataset\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# labels\n",
    "print(y_train)\n",
    "\n",
    "# data\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def see_images(dataset, n):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(dataset[i].reshape(28,28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_images(x_train, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to preprocess our data to use them as input in the network. \n",
    "# We normalise the images, each pixel within [0, 1] and format the images into a vector\n",
    "\n",
    "x_train = x_train.astype('float32') / 255 # 255 max rgb value\n",
    "x_test = x_test.astype('float32') / 255\n",
    "x_train = x_train.reshape(len(x_train), np.prod(x_train.shape[1:]))\n",
    "x_test = x_test.reshape(len(x_test), np.prod(x_test.shape[1:]))\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "# data\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplest autoencoder\n",
    "We will use a single fully-connected neural for our encoder and decoder.\n",
    "\n",
    "Elements:\n",
    "\n",
    "Input Layer\n",
    "\n",
    "* Input layer: $y = xI$\n",
    "\n",
    "Encoder\n",
    "\n",
    "* Fully-connected layer: $y = xA^T + b$\n",
    "* Rectified Linear Unit function: $ReLu(x)=max(0, x)$ (encoder)\n",
    "\n",
    "Decoder\n",
    "\n",
    "* Fully-connected layer: $y = xW^T + b$\n",
    "* Sigmoid function: $Sigmoid(x) = \\sigma(x) = \\frac{1}{1+exp(-x)}$ (decoder)\n",
    "\n",
    "![autoencoder for MNIST](https://blog.keras.io/img/ae/autoencoder_schema.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model # (template for architecture)\n",
    "\n",
    "# latent representations dimension\n",
    "latent_dim = 32\n",
    "# input layer\n",
    "input_img = Input(shape=(input_dim, ))\n",
    "\n",
    "# encoder\n",
    "encoder = Dense(latent_dim, activation='relu')\n",
    "encoder_feats = encoder(input_img)\n",
    "\n",
    "# decoder\n",
    "decoder = Dense(input_dim, activation='sigmoid')\n",
    "decoder_out = decoder(encoder_feats)\n",
    "\n",
    "# model\n",
    "simple_autoencoder = Model(input_img, decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(input_img, encoder_feats)\n",
    "encoded_input = Input(shape=(latent_dim, ))\n",
    "decoder_model = Model(encoded_input, decoder(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, we need to set up the loss function and optimizer.\n",
    "\n",
    "Loss function per-pixel crossentropy loss\n",
    "\n",
    "$y' = p(y)$\n",
    "\n",
    "$loss = -[y * log(y' + \\epsilon) + (1-y) * log(1-y' + \\epsilon)]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "simple_autoencoder.fit(x_train, x_train,  # the target is the same input data!\n",
    "                       epochs=50, \n",
    "                       batch_size=256, \n",
    "                       shuffle=True, \n",
    "                       validation_data=(x_test, x_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict digits\n",
    "prediction = simple_autoencoder.predict(x_test)\n",
    "\n",
    "# Visualisation of predictions\n",
    "see_images(x_test, 5)\n",
    "see_images(prediction, 5)\n",
    "\n",
    "# Using decoder and encoder model\n",
    "encoded_imgs = encoder_model.predict(x_test)\n",
    "decoded_imgs = decoder_model.predict(encoded_imgs)\n",
    "\n",
    "see_images(decoded_imgs, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent representations\n",
    "encoded_imgs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varietional Autoencoder (VAE)\n",
    "\n",
    "VAE is a generative model that learns the parameters of the probability distribution modelling the input data. \n",
    "\n",
    "\"A VAE encodes data Y (e.g., a sentence) as hidden random variables Z, based on which the decoder reconstructs Y. Consider a generative model, parameterized by $\\theta$, as\"\n",
    "\n",
    "$p_{\\theta}(Z,Y) = p_{\\theta}(Z)p_{\\theta}(Y|Z)$ (Bahuleyan et al., 2018)\n",
    "\n",
    "**Encoder**: it learns two parameters (`z_mean`, and `z_log_sigma`) in the latent space from the input data, and it randomly samples points from the latent normal distribution (`z = z_mean + exp(0.5 * z_log_sigma) * epsilon`).\n",
    "\n",
    "**Decoder**: it maps the latent space points to the original input data\n",
    "\n",
    "\n",
    "**Loss fuctions**: a VAE has two loss functions: the unimodal(reconstruction $l_{re}$) loss as in the simple autoencoder and the KL divergence ($D_{KL}$) between the latent distribution (approximation, learned parameters) and the prior distribution (actual distribution of latent space ?) which works as a regulaser. \n",
    "\n",
    "$l_{vae} = l_{re} - D_{KL}$\n",
    "\n",
    "$l_{vae} = -[y * log(y' + \\epsilon) + (1-y) * log(1-y' + \\epsilon)] - [0.5 * \\frac{1}{n}\\sum^{n}{1+z_{sigma} - z_{mean}^2 - e^{z_{sigma}}}]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "intermediate_dim = 512 # dimension of points in the latent space\n",
    "batch_size = 128 \n",
    "latent_dim = 2 # the two latent parameters z_mean and z_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![architecture sequential VAE](img/VAE_arch.png)\n",
    "(Kingma and Welling, 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "\n",
    "inputs = Input(shape=(input_dim, ), name='encoder_input')\n",
    "\n",
    "# encoder (x -> z_mean & x -> z_log_var) A 'Y' shape\n",
    "encoder_1 = Dense(intermediate_dim, activation='relu')\n",
    "encoder_out_1 = encoder_1(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')\n",
    "z_mean_out = z_mean(encoder_out_1)\n",
    "z_log_sigma = Dense(latent_dim, name='z_log_sigma')\n",
    "z_log_sigma_out = z_log_sigma(encoder_out_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "from keras import backend as K # operations with tensors\n",
    "\n",
    "def sampling(params):\n",
    "    z_mean, z_log_sigma = params\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim)) # mean = 0, std = 1\n",
    "    return z_mean + K.exp(0.5 * z_log_sigma) * epsilon #\n",
    "\n",
    "# We need to wrap the output of sampling into a layer for connecting it with the decoder. For that we can\n",
    "# a Lambda layer\n",
    "from keras.layers import Lambda\n",
    "z = Lambda(sampling, output_shape=(latent_dim, ))\n",
    "z_out = z([z_mean_out, z_log_sigma_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "decoder_1 = Dense(intermediate_dim, activation='relu')\n",
    "decoder_2 = Dense(input_dim, activation='sigmoid')\n",
    "decoder_1_out = decoder_1(z_out)\n",
    "decoder_2_out = decoder_2(decoder_1_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Models](img/vae_ex_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "\n",
    "# end-to-end autoencoder \n",
    "vae = Model(?, ?) \n",
    "\n",
    "# from inputs to latent space\n",
    "encoder_model = Model(?, ?) \n",
    "\n",
    "# generator\n",
    "decoder_input = Input(shape=(dim?, ))\n",
    "generator = Model(decoder_input, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "# Reconstruction loss and KL divergence\n",
    "\n",
    "l_re = binary_crossentropy(inputs, decoder_2_out) * input_dim\n",
    "d_kl = - 0.5 * K.sum(1 + z_log_sigma_out - K.square(z_mean_out) - K.exp(z_log_sigma_out), axis=-1)\n",
    "\n",
    "loss = K.mean(l_re + d_kl)\n",
    "\n",
    "vae.add_loss(loss)\n",
    "\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "vae.fit(x_train,\n",
    "       shuffle=True,\n",
    "       epochs=20,\n",
    "       batch_size=batch_size,\n",
    "       validation_data=(x_test, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder_model.predict(x_test, batch_size=batch_size)\n",
    "\n",
    "# distributions of the different classes\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"z[0]\")\n",
    "plt.ylabel(\"z[1]\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display generator usin random values within the interval\n",
    "# shown by the encoder [-4,4]\n",
    "\n",
    "n = 12  # figure with 12x12 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "# we will sample n points within [-4, 4] standard deviations\n",
    "grid_x = np.linspace(-4, 4, n) # value obtain with the encoder\n",
    "grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = generator.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# axis labels \n",
    "start_range = digit_size // 2\n",
    "end_range = (n - 1) * digit_size + start_range + 1\n",
    "\n",
    "pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "\n",
    "sample_range_x = np.round(grid_x, 1)\n",
    "sample_range_y = np.round(grid_y, 1)\n",
    "\n",
    "plt.xticks(pixel_range, sample_range_x)\n",
    "plt.yticks(pixel_range, sample_range_y)\n",
    "plt.xlabel(\"z[0]\")\n",
    "plt.ylabel(\"z[1]\")\n",
    "\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
