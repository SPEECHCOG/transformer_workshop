{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classifier\n",
    "Tutorial from [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/) by Prodip Hore and Sayan Chatterjee\n",
    "\n",
    "## Dataset\n",
    "\n",
    "UCI Machine Learning Repository: Sentiment Labelled Sentences Data Set\n",
    "('From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015)\n",
    "\n",
    "Sentences: 2000 (We are only using Amazon and Yelp files)\n",
    "\n",
    "Labels: Positive (1) - Negative (0)\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "* \"The mic is great.\" Positive ->  `The mic is great.\t1`\n",
    "\n",
    "* \"What a waste of money and time!.\" Negative -> `What a waste of money and time!.\t0`\n",
    "\n",
    "\n",
    "## Architecture\n",
    "\n",
    "Input layer -> Embedding layer -> LSTM -> Dense (softmax) -> Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read txt files\n",
    "with open('data/amazon.txt', mode='r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "with open('data/yelp.txt', mode='r') as f:\n",
    "    lines += f.readlines()\n",
    "\n",
    "# Split lines so we have sentences and the class as an integer\n",
    "sentences = [line.split('\\t')[0] for line in lines]\n",
    "labels = [int(line.split('\\t')[1]) for line in lines]\n",
    "labels = np.asarray(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: So there is no way for me to plug it in here in the US unless I go by a converter.\n",
      "representation: \n",
      "[27, 58, 7, 55, 141, 12, 60, 6, 268, 5, 14, 45, 14, 1, 148, 448, 3, 59, 112, 4, 1427]\n",
      "max length: 32\n",
      "vocabulary size: 3259\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenizer: An object with an internal lexicon, and unknown token.\n",
    "t = Tokenizer()\n",
    "# Load the dataset in the tokenizer\n",
    "t.fit_on_texts(sentences)\n",
    "\n",
    "# Maps the words in the sentences with the indeces in the lexicon (list of lists)\n",
    "text_matrix= t.texts_to_sequences(sentences)\n",
    "\n",
    "print('sentence: ' + sentences[0])\n",
    "\n",
    "print('representation: ')\n",
    "print(text_matrix[0])\n",
    "\n",
    "\n",
    "# calculate max length of sentence in the corpus\n",
    "max_length = 0\n",
    "\n",
    "for i in range(len(text_matrix)):\n",
    "    sent_length = len(text_matrix[i])\n",
    "    if max_length < sent_length:\n",
    "        max_length = sent_length\n",
    "    \n",
    "print('max length: %d' % max_length)\n",
    "\n",
    "# The vocabulary size will be determine by the index of the last word in the lexicon (index starting from 0)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "print('vocabulary size: %d'%vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "1600\n",
      "400\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# dimension of the embeddings to represent the words with vectors of the same dimension. \n",
    "emb_dim = 16\n",
    "\n",
    "# we need to pad the sentences that have less words than the maximum length by adding zeros\n",
    "tex_pad = pad_sequences(text_matrix, maxlen=max_length, padding='post')\n",
    "\n",
    "# Dummy train test sets split \n",
    "x_train = tex_pad[:1600,:]\n",
    "y_train = labels[:1600]\n",
    "x_test = tex_pad[1600:,:]\n",
    "y_test = labels[1600:]\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape: \n",
      "(None, 32, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 32)\n",
      "attention weights shape: \n",
      "(None, 32)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 32, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"bahdanau_attention/mul:0\", shape=(None, 32, 10), dtype=float32)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 32, 16)            52144     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32, 10)            1080      \n",
      "_________________________________________________________________\n",
      "bahdanau_attention (Bahdanau ((None, 10), (None, 32, 1 42        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 53,288\n",
      "Trainable params: 53,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Layer, GlobalAveragePooling1D, Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Custom attetion layer\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BahdanauAttention, self).__init__(**kwargs)\n",
    "    \n",
    "    # This method states the weights that the layer will learn. It has as input param the shape of the input\n",
    "    # which is called. This method is called at the declaration time\n",
    "    def build(self, input_shape):\n",
    "        # We need to provide the dimensions of our weights. In this example, we will have a W_a matrix of\n",
    "        # dimension (lstm_units, 1), and a bias of dimension (max_length, 1)\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(BahdanauAttention, self).build(input_shape)\n",
    "    \n",
    "    # In this method with do all the calculations of the layer and return the output of the layer\n",
    "    def call(self, x):\n",
    "        # x is the input of the layer. In this example, the output of lstm (hidden_statesxlstm_units) \n",
    "        # hidden_states = max_length\n",
    "        \n",
    "        # We calculate the score tanh(W.x + b)\n",
    "        scores = K.tanh(K.dot(x,self.W)+self.b)  # (max_length x 1) \n",
    "        print('scores shape: ')\n",
    "        print(scores.shape)\n",
    "        \n",
    "        # This removes the last axis -> a vector of max_length dimension \n",
    "        # we can omit this since our W matrix has dimension 1 in the last axis\n",
    "        scores=K.squeeze(scores, axis=-1) \n",
    "        print('scores shape after squeeze: ')\n",
    "        print(scores.shape)\n",
    "        \n",
    "        # we apply softmax (the last axis is the default axis used for calculation)\n",
    "        at=K.softmax(scores)\n",
    "        print('attention weights shape: ')\n",
    "        print(at.shape)\n",
    "        \n",
    "        # This adds a 1-sized dimension to the last axis -> matrix of (max_length x 1)\n",
    "        at=K.expand_dims(at,axis=-1) # if there is no squeeze, then we can omit this\n",
    "        print('attention weights shape after expand_dims: ')\n",
    "        print(at.shape)\n",
    "        \n",
    "        # We calculate the weighted values -> \\alpha*hidden_states         \n",
    "        # row-wise multiplication (we are weighting the hidden_states, not the lstm_units) \n",
    "        output=x*at # (max_length x lstm_units)\n",
    "        print('weighted values shape: ')\n",
    "        print(output)\n",
    "        \n",
    "        # The output of this layer is the weighted values (we sum up the values of the hidden states), and\n",
    "        # the weights of the attetnion (max_length x 1)\n",
    "        return K.sum(output, axis=1), at\n",
    "    \n",
    "    # This is used for summary, to see the output shape of the two output matrices\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "    \n",
    "    # This is used for summary (it returns the params of the layer)\n",
    "    def get_config(self):\n",
    "        return super(BahdanauAttention, self).get_config()\n",
    "\n",
    "\n",
    "# Architecture\n",
    "lstm_units = 10\n",
    "\n",
    "inputs = Input(shape=(max_length,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=max_length, embeddings_regularizer=l2(.001))\n",
    "embd_out = embedding(inputs)\n",
    "lstm = LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.2, return_sequences=True)\n",
    "lstm_out = lstm(embd_out)\n",
    "\n",
    "weigthed_out, weights = BahdanauAttention()(lstm_out)\n",
    "\n",
    "prob = Dense(2, activation='sigmoid')\n",
    "outputs = prob(weigthed_out)\n",
    "\n",
    "model = Model(inputs, outputs) # classifier\n",
    "attention_model = Model(inputs, weights) # attention weights\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "scores shape: \n",
      "(32, 32, 1)\n",
      "scores shape after squeeze: \n",
      "(32, 32)\n",
      "attention weights shape: \n",
      "(32, 32)\n",
      "attention weights shape after expand_dims: \n",
      "(32, 32, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model/bahdanau_attention/mul:0\", shape=(32, 32, 10), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/workshop/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape: \n",
      "(32, 32, 1)\n",
      "scores shape after squeeze: \n",
      "(32, 32)\n",
      "attention weights shape: \n",
      "(32, 32)\n",
      "attention weights shape after expand_dims: \n",
      "(32, 32, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model/bahdanau_attention/mul:0\", shape=(32, 32, 10), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/workshop/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1600 [============================>.] - ETA: 0s - loss: 0.7060 - acc: 0.5166scores shape: \n",
      "(None, 32, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 32)\n",
      "attention weights shape: \n",
      "(None, 32)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 32, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model/bahdanau_attention/mul:0\", shape=(None, 32, 10), dtype=float32)\n",
      "1600/1600 [==============================] - 3s 2ms/sample - loss: 0.7056 - acc: 0.5188 - val_loss: 0.6986 - val_acc: 0.4125\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 0.6909 - acc: 0.5219 - val_loss: 0.7006 - val_acc: 0.4150\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 0.6757 - acc: 0.5731 - val_loss: 0.6417 - val_acc: 0.7150\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 0.5265 - acc: 0.8100 - val_loss: 0.5349 - val_acc: 0.7625\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 0.4141 - acc: 0.8550 - val_loss: 0.5671 - val_acc: 0.7500\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 0.3568 - acc: 0.8856 - val_loss: 0.5705 - val_acc: 0.7625\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 0.2810 - acc: 0.9219 - val_loss: 0.5498 - val_acc: 0.7850\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 0.2536 - acc: 0.9312 - val_loss: 0.5624 - val_acc: 0.7975\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 0.2398 - acc: 0.9344 - val_loss: 0.5702 - val_acc: 0.7925\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 0.2655 - acc: 0.9162 - val_loss: 0.5714 - val_acc: 0.7950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f36580795c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "\n",
    "model.fit(x=x_train,y=y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_test,y_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i miss it and wish they had one in philadelphia', 'we got sitting fairly fast but ended up waiting 40 minutes just to place our order another 30 minutes before the food arrived', 'they also have the best cheese crisp in town', 'good value great food great service', \"couldn't ask for a more satisfying meal\", 'the food is good', 'it was awesome', 'i just wanted to leave', 'we made the drive all the way from north scottsdale and i was not one bit disappointed', 'i will not be eating there again']\n",
      "[1 0 1 1 1 1 1 0 1 0]\n",
      "scores shape: \n",
      "(None, 32, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 32)\n",
      "attention weights shape: \n",
      "(None, 32)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 32, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model/bahdanau_attention/mul:0\", shape=(None, 32, 10), dtype=float32)\n",
      "[[0.6030605  0.15063035]\n",
      " [0.63111967 0.11287954]\n",
      " [0.05503449 0.9607009 ]\n",
      " [0.04045883 0.97273684]\n",
      " [0.68081427 0.05628744]\n",
      " [0.04759791 0.9668544 ]\n",
      " [0.05885363 0.9574808 ]\n",
      " [0.66232026 0.0722383 ]\n",
      " [0.618875   0.11932856]\n",
      " [0.6982858  0.04156557]]\n"
     ]
    }
   ],
   "source": [
    "# The model barely learnt. Results change with each execusion\n",
    "# acc_train = 0.56 last epoch vs acc_train = 0.52 first epoch\n",
    "# acc_test = 0.45 last epoch vs acc_test = 0.41 for first epoch\n",
    "# Test\n",
    "print(t.sequences_to_texts(x_test[:10]))\n",
    "print(y_test[:10])\n",
    "\n",
    "pred = model.predict(x_test[:10])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Input layer -> Embedding block -> Transforemer Block -> GlobalAveragePooling -> Dropout ->Dense (reduce dimensions) -> Dropout -> Dense (softmax) -> Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Layer, Embedding, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "    \n",
    "# Multi-head self-attention\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value, mask):\n",
    "        score = tf.matmul(query, key, transpose_b=True) # (batch_size, )\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # d_model\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        if mask:\n",
    "            # look ahead mask\n",
    "            seq_length = tf.shape(key)[1]\n",
    "            mask_matrix = 1 - tf.linalg.band_part((tf.ones(seq_length, seq_length)), -1, 0)\n",
    "            scaled_score += mask_matrix * -1e9 # -inf for masked values         \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs, mask=False):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value, mask)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
    "        ) # FeedForward Network\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        # if training=True the layer add dropout, in inference mode it does nothing.\n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) # Residual connection (+)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- dimensions -----------------------\n",
      "Shape embedding layer: \n",
      "(None, 32, 16)\n",
      "Shape Transformer block: \n",
      "(None, 32, 16)\n",
      "Shape global average pooling layer: \n",
      "(None, 16)\n",
      "Shape dropout layer: \n",
      "(None, 16)\n",
      "Shape dense 1 layer: \n",
      "(None, 20)\n",
      "Shape dropout layer: \n",
      "(None, 20)\n",
      "Shape dense 2 (output) layer: \n",
      "(None, 2)\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 32, 16)            52656     \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 32, 16)            1498      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                340       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 54,536\n",
      "Trainable params: 54,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "embed_dim = 16  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 10  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = Input(shape=(max_length,))\n",
    "embedding_layer = TokenAndPositionEmbedding(max_length, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "print('----------------------- dimensions -----------------------')\n",
    "print('Shape embedding layer: ')\n",
    "print(x.shape)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "print('Shape Transformer block: ')\n",
    "print(x.shape)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "print('Shape global average pooling layer: ')\n",
    "print(x.shape)\n",
    "x = Dropout(0.1)(x)\n",
    "print('Shape dropout layer: ')\n",
    "print(x.shape)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "print('Shape dense 1 layer: ')\n",
    "print(x.shape)\n",
    "x = Dropout(0.1)(x)\n",
    "print('Shape dropout layer: ')\n",
    "print(x.shape)\n",
    "outputs = Dense(2, activation=\"softmax\")(x)\n",
    "print('Shape dense 2 (output) layer: ')\n",
    "print(outputs.shape)\n",
    "\n",
    "model_transformer = Model(inputs=inputs, outputs=outputs)\n",
    "print(model_transformer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 0.7043 - acc: 0.5325 - val_loss: 0.7000 - val_acc: 0.4725\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 1s 494us/sample - loss: 0.6436 - acc: 0.6338 - val_loss: 0.6322 - val_acc: 0.5700\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 1s 448us/sample - loss: 0.4538 - acc: 0.7906 - val_loss: 0.5112 - val_acc: 0.7450\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 1s 500us/sample - loss: 0.1915 - acc: 0.9250 - val_loss: 0.6232 - val_acc: 0.7350\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 1s 436us/sample - loss: 0.0970 - acc: 0.9681 - val_loss: 0.6375 - val_acc: 0.7700\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 1s 448us/sample - loss: 0.0688 - acc: 0.9775 - val_loss: 0.8715 - val_acc: 0.7400\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 1s 429us/sample - loss: 0.0559 - acc: 0.9806 - val_loss: 1.2189 - val_acc: 0.6900\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 1s 782us/sample - loss: 0.0344 - acc: 0.9894 - val_loss: 0.9556 - val_acc: 0.7600\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 1s 450us/sample - loss: 0.0388 - acc: 0.9862 - val_loss: 1.0342 - val_acc: 0.7325\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 1s 557us/sample - loss: 0.0233 - acc: 0.9912 - val_loss: 0.9857 - val_acc: 0.7850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f360083e128>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transformer.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_transformer.fit(x=x_train,y=y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_test,y_test)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 32 108 819 756 331  28 661  52 425 727 124  50   6  26  78 198 209 592\n",
      " 124 205   1  24 364   0   0   0   0   0   0   0   0   0]\n",
      "['we got sitting fairly fast but ended up waiting 40 minutes just to place our order another 30 minutes before the food arrived']\n",
      "[0.9899026  0.01009738]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "\n",
    "pred = model_transformer.predict(x_test)\n",
    "\n",
    "\n",
    "print(x_test[1])\n",
    "print(t.sequences_to_texts(x_test[1:2]))\n",
    "print(pred[1])\n",
    "print(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
