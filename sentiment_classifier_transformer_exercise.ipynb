{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classifier\n",
    "Tutorial from [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/) by Prodip Hore and Sayan Chatterjee\n",
    "\n",
    "## Dataset\n",
    "\n",
    "UCI Machine Learning Repository: Sentiment Labelled Sentences Data Set\n",
    "('From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015)\n",
    "\n",
    "Sentences: 2000 (We are only using Amazon and Yelp files)\n",
    "\n",
    "Labels: Positive (1) - Negative (0)\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "* \"The mic is great.\" Positive ->  `The mic is great.\t1`\n",
    "\n",
    "* \"What a waste of money and time!.\" Negative -> `What a waste of money and time!.\t0`\n",
    "\n",
    "\n",
    "## Architecture\n",
    "\n",
    "Input layer -> Embedding layer -> LSTM -> Dense (softmax) -> Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read txt files\n",
    "with open('data/amazon.txt', mode='r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "with open('data/yelp.txt', mode='r') as f:\n",
    "    lines += f.readlines()\n",
    "\n",
    "# Split lines so we have sentences and the class as an integer\n",
    "sentences = [line.split('\\t')[0] for line in lines]\n",
    "labels = [int(line.split('\\t')[1]) for line in lines]\n",
    "labels = np.asarray(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenizer: An object with an internal lexicon, and unknown token.\n",
    "t = Tokenizer()\n",
    "# Load the dataset in the tokenizer\n",
    "t.fit_on_texts(sentences)\n",
    "\n",
    "# Maps the words in the sentences with the indeces in the lexicon (list of lists)\n",
    "text_matrix= t.texts_to_sequences(sentences)\n",
    "\n",
    "print('sentence: ' + sentences[0])\n",
    "\n",
    "print('representation: ')\n",
    "print(text_matrix[0])\n",
    "\n",
    "\n",
    "# calculate max length of sentence in the corpus\n",
    "max_length = 0\n",
    "\n",
    "for i in range(len(text_matrix)):\n",
    "    sent_length = len(text_matrix[i])\n",
    "    if max_length < sent_length:\n",
    "        max_length = sent_length\n",
    "    \n",
    "print('max length: %d' % max_length)\n",
    "\n",
    "# The vocabulary size will be determine by the index of the last word in the lexicon (index starting from 0)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "print('vocabulary size: %d'%vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# dimension of the embeddings to represent the words with vectors of the same dimension. \n",
    "emb_dim = 16\n",
    "\n",
    "# we need to pad the sentences that have less words than the maximum length by adding zeros\n",
    "tex_pad = pad_sequences(text_matrix, maxlen=max_length, padding='post')\n",
    "\n",
    "# Dummy train test sets split \n",
    "x_train = tex_pad[:1600,:]\n",
    "y_train = labels[:1600]\n",
    "x_test = tex_pad[1600:,:]\n",
    "y_test = labels[1600:]\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Layer, GlobalAveragePooling1D, Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Custom attetion layer\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BahdanauAttention, self).__init__(**kwargs)\n",
    "    \n",
    "    # This method states the weights that the layer will learn. It has as input param the shape of the input\n",
    "    # which is called. This method is called at the declaration time\n",
    "    def build(self, input_shape):\n",
    "        # We need to provide the dimensions of our weights. In this example, we will have a W_a matrix of\n",
    "        # dimension (lstm_units, 1), and a bias of dimension (max_length, 1)\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(BahdanauAttention, self).build(input_shape)\n",
    "    \n",
    "    # In this method with do all the calculations of the layer and return the output of the layer\n",
    "    def call(self, x):\n",
    "        # x is the input of the layer. In this example, the output of lstm (hidden_statesxlstm_units) \n",
    "        # hidden_states = max_length\n",
    "        \n",
    "        # We calculate the score tanh(W.x + b)\n",
    "        scores = K.tanh(K.dot(x,self.W)+self.b)  # (max_length x 1) \n",
    "        print('scores shape: ')\n",
    "        print(scores.shape)\n",
    "        \n",
    "        # This removes the last axis -> a vector of max_length dimension \n",
    "        # we can omit this since our W matrix has dimension 1 in the last axis\n",
    "        scores=K.squeeze(scores, axis=-1) \n",
    "        print('scores shape after squeeze: ')\n",
    "        print(scores.shape)\n",
    "        \n",
    "        # we apply softmax (the last axis is the default axis used for calculation)\n",
    "        at=K.softmax(scores)\n",
    "        print('attention weights shape: ')\n",
    "        print(at.shape)\n",
    "        \n",
    "        # This adds a 1-sized dimension to the last axis -> matrix of (max_length x 1)\n",
    "        at=K.expand_dims(at,axis=-1) # if there is no squeeze, then we can omit this\n",
    "        print('attention weights shape after expand_dims: ')\n",
    "        print(at.shape)\n",
    "        \n",
    "        # We calculate the weighted values -> \\alpha*hidden_states         \n",
    "        # row-wise multiplication (we are weighting the hidden_states, not the lstm_units) \n",
    "        output=x*at # (max_length x lstm_units)\n",
    "        print('weighted values shape: ')\n",
    "        print(output)\n",
    "        \n",
    "        # The output of this layer is the weighted values (we sum up the values of the hidden states), and\n",
    "        # the weights of the attetnion (max_length x 1)\n",
    "        return K.sum(output, axis=1), at\n",
    "    \n",
    "    # This is used for summary, to see the output shape of the two output matrices\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "    \n",
    "    # This is used for summary (it returns the params of the layer)\n",
    "    def get_config(self):\n",
    "        return super(BahdanauAttention, self).get_config()\n",
    "\n",
    "\n",
    "# Architecture\n",
    "lstm_units = 10\n",
    "\n",
    "inputs = Input(shape=(max_length,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=max_length, embeddings_regularizer=l2(.001))\n",
    "embd_out = embedding(inputs)\n",
    "lstm = LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.2, return_sequences=True)\n",
    "lstm_out = lstm(embd_out)\n",
    "\n",
    "weigthed_out, weights = BahdanauAttention()(lstm_out)\n",
    "\n",
    "prob = Dense(2, activation='sigmoid')\n",
    "outputs = prob(weigthed_out)\n",
    "\n",
    "model = Model(inputs, outputs) # classifier\n",
    "attention_model = Model(inputs, weights) # attention weights\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "\n",
    "model.fit(x=x_train,y=y_train,\n",
    "          batch_size=100,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_test,y_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model barely learnt. Results change with each execusion\n",
    "# acc_train = 0.56 last epoch vs acc_train = 0.52 first epoch\n",
    "# acc_test = 0.45 last epoch vs acc_test = 0.41 for first epoch\n",
    "# Test\n",
    "print(t.sequences_to_texts(x_test[:10]))\n",
    "print(y_test[:10])\n",
    "\n",
    "pred = model.predict(x_test[:10])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Input layer -> Embedding block -> Transforemer Block -> GlobalAveragePooling -> Dropout ->Dense (reduce dimensions) -> Dropout -> Dense (softmax) -> Label\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Create the Embedding and Transfomer block (copy code from text_classifier_transformer)\n",
    "1. Create classifier model. This time, to have a model size similar to the LSTM model, used embedding size of 16 amd ffn dimension of 10\n",
    "1. Use the same training parameters as before (epochs, batch, etc) and compare behaviours\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Layer, Embedding, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# TokenAndPositionEmbedding class\n",
    "\n",
    "# MultiHeadSelfAttention class\n",
    "\n",
    "# TransformerBlock class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# define the classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model predictions\n",
    "\n",
    "# pred = model_transformer.predict(x_test)\n",
    "\n",
    "\"\"\"\n",
    "print(x_test[1])\n",
    "print(t.sequences_to_texts(x_test[1:2]))\n",
    "print(pred[1])\n",
    "print(y_test[1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
