{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classifier\n",
    "Tutorial from [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/) by Prodip Hore and Sayan Chatterjee\n",
    "\n",
    "## Dataset\n",
    "\n",
    "UCI Machine Learning Repository: Sentiment Labelled Sentences Data Set\n",
    "('From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015)\n",
    "\n",
    "Sentences: 2000 (We are only using Amazon and Yelp files)\n",
    "\n",
    "Labels: Positive (1) - Negative (0)\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "* \"The mic is great.\" Positive ->  `The mic is great.\t1`\n",
    "\n",
    "* \"What a waste of money and time!.\" Negative -> `What a waste of money and time!.\t0`\n",
    "\n",
    "\n",
    "## Architecture\n",
    "\n",
    "Input layer -> Embedding layer -> LSTM -> Dense (softmax) -> Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read txt files\n",
    "with open('data/amazon.txt', mode='r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "with open('data/yelp.txt', mode='r') as f:\n",
    "    lines += f.readlines()\n",
    "\n",
    "# Split lines so we have sentences and the class as an integer\n",
    "sentences = [line.split('\\t')[0] for line in lines]\n",
    "labels = [int(line.split('\\t')[1]) for line in lines]\n",
    "labels = np.asarray(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: So there is no way for me to plug it in here in the US unless I go by a converter.\n",
      "representation: \n",
      "[27, 58, 7, 55, 141, 12, 60, 6, 268, 5, 14, 45, 14, 1, 148, 448, 3, 59, 112, 4, 1427]\n",
      "max length: 32\n",
      "vocabulary size: 3259\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenizer: An object with an internal lexicon, and unknown token.\n",
    "t = Tokenizer()\n",
    "# Load the dataset in the tokenizer\n",
    "t.fit_on_texts(sentences)\n",
    "\n",
    "# Maps the words in the sentences with the indeces in the lexicon (list of lists)\n",
    "text_matrix= t.texts_to_sequences(sentences)\n",
    "\n",
    "print('sentence: ' + sentences[0])\n",
    "\n",
    "print('representation: ')\n",
    "print(text_matrix[0])\n",
    "\n",
    "\n",
    "# calculate max length of sentence in the corpus\n",
    "max_length = 0\n",
    "\n",
    "for i in range(len(text_matrix)):\n",
    "    sent_length = len(text_matrix[i])\n",
    "    if max_length < sent_length:\n",
    "        max_length = sent_length\n",
    "    \n",
    "print('max length: %d' % max_length)\n",
    "\n",
    "# The vocabulary size will be determine by the index of the last word in the lexicon (index starting from 0)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "print('vocabulary size: %d'%vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "1600\n",
      "400\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# dimension of the embeddings to represent the words with vectors of the same dimension. \n",
    "emb_dim = 16\n",
    "\n",
    "# we need to pad the sentences that have less words than the maximum length by adding zeros\n",
    "tex_pad = pad_sequences(text_matrix, maxlen=max_length, padding='post')\n",
    "\n",
    "# Dummy train test sets split \n",
    "x_train = tex_pad[:1600,:]\n",
    "y_train = labels[:1600]\n",
    "x_test = tex_pad[1600:,:]\n",
    "y_test = labels[1600:]\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 32, 16)            52144     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10)                1080      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 53,235\n",
      "Trainable params: 53,235\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "lstm_units = 10\n",
    "\n",
    "inputs = Input(shape=(max_length,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=max_length, embeddings_regularizer=l2(.001))\n",
    "embd_out = embedding(inputs)\n",
    "lstm = LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.2)\n",
    "lstm_out = lstm(embd_out)\n",
    "\n",
    "prob = Dense(1, activation='sigmoid')\n",
    "outputs = prob(lstm_out)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/workshop/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 2s 2ms/sample - loss: 0.7207 - acc: 0.5206 - val_loss: 0.7136 - val_acc: 0.4125\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 1s 366us/sample - loss: 0.7007 - acc: 0.5225 - val_loss: 0.7067 - val_acc: 0.4125\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 1s 341us/sample - loss: 0.6938 - acc: 0.5219 - val_loss: 0.7038 - val_acc: 0.4125\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 1s 456us/sample - loss: 0.6925 - acc: 0.5206 - val_loss: 0.7001 - val_acc: 0.4125\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 1s 456us/sample - loss: 0.6923 - acc: 0.5219 - val_loss: 0.7025 - val_acc: 0.4125\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 1s 368us/sample - loss: 0.6919 - acc: 0.5225 - val_loss: 0.7017 - val_acc: 0.4125\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 1s 470us/sample - loss: 0.6915 - acc: 0.5238 - val_loss: 0.7014 - val_acc: 0.4125\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 1s 410us/sample - loss: 0.6919 - acc: 0.5238 - val_loss: 0.6993 - val_acc: 0.4175\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 1s 368us/sample - loss: 0.6920 - acc: 0.5294 - val_loss: 0.6989 - val_acc: 0.4200\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 1s 398us/sample - loss: 0.6859 - acc: 0.5556 - val_loss: 0.6981 - val_acc: 0.4475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd8547b12b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "\n",
    "model.fit(x=x_train,y=y_train,\n",
    "          batch_size=100,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_test,y_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i miss it and wish they had one in philadelphia', 'we got sitting fairly fast but ended up waiting 40 minutes just to place our order another 30 minutes before the food arrived', 'they also have the best cheese crisp in town', 'good value great food great service', \"couldn't ask for a more satisfying meal\", 'the food is good', 'it was awesome', 'i just wanted to leave', 'we made the drive all the way from north scottsdale and i was not one bit disappointed', 'i will not be eating there again']\n",
      "[1 0 1 1 1 1 1 0 1 0]\n",
      "[[0.5299712 ]\n",
      " [0.49581152]\n",
      " [0.54338825]\n",
      " [0.56598663]\n",
      " [0.53870577]\n",
      " [0.5556019 ]\n",
      " [0.54109323]\n",
      " [0.5389493 ]\n",
      " [0.5079776 ]\n",
      " [0.5250999 ]]\n"
     ]
    }
   ],
   "source": [
    "# The model barely learnt. Results change with each execusion\n",
    "# acc_train = 0.56 last epoch vs acc_train = 0.52 first epoch\n",
    "# acc_test = 0.45 last epoch vs acc_test = 0.41 for first epoch\n",
    "# Test\n",
    "print(t.sequences_to_texts(x_test[:10]))\n",
    "print(y_test[:10])\n",
    "\n",
    "pred = model.predict(x_test[:10])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Input layer -> Embedding layer -> LSTM -> Attention -> Dense (softmax) -> Label\n",
    "\n",
    "### Attention (Bahdanau et al., 2015)\n",
    "Additive Attention\n",
    "\n",
    "1. $\\large score(s_t, h_i) = v_a^T \\text{tanh}(W_a[s_t;h_i])$ \n",
    "\n",
    "   -> In this example we use local attention $score(h) = \\large \\text{tanh}(W_ah + b)$\n",
    "\n",
    "2. $\\large \\alpha_{ti}=\\frac{exp(score_{ti})}{\\sum_{k=1}{N}{exp(score_{tk})}}$\n",
    "\n",
    "3. $\\large \\alpha \\cdot h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Custom attetion layer\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BahdanauAttention, self).__init__(**kwargs)\n",
    "    \n",
    "    # This method states the weights that the layer will learn. It has as input param the shape of the input\n",
    "    # which is called. This method is called at the declaration time\n",
    "    def build(self, input_shape):\n",
    "        # We need to provide the dimensions of our weights. In this example, we will have a W_a matrix of\n",
    "        # dimension (lstm_units, 1), and a bias of dimension (max_length, 1)\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(BahdanauAttention, self).build(input_shape)\n",
    "    \n",
    "    # In this method with do all the calculations of the layer and return the output of the layer\n",
    "    def call(self, x):\n",
    "        # x is the input of the layer. In this example, the output of lstm (hidden_statesxlstm_units) \n",
    "        # hidden_states = max_length\n",
    "        \n",
    "        # We calculate the score tanh(W.x + b)\n",
    "        scores = K.tanh(K.dot(x,self.W)+self.b)  # (max_length x 1) \n",
    "        print('scores shape: ')\n",
    "        print(scores.shape)\n",
    "        \n",
    "        # This removes the last axis -> a vector of max_length dimension \n",
    "        # we can omit this since our W matrix has dimension 1 in the last axis\n",
    "        scores=K.squeeze(scores, axis=-1) \n",
    "        print('scores shape after squeeze: ')\n",
    "        print(scores.shape)\n",
    "        \n",
    "        # we apply softmax (the last axis is the default axis used for calculation)\n",
    "        at=K.softmax(scores)\n",
    "        print('attention weights shape: ')\n",
    "        print(at.shape)\n",
    "        \n",
    "        # This adds a 1-sized dimension to the last axis -> matrix of (max_length x 1)\n",
    "        at=K.expand_dims(at,axis=-1) # if there is no squeeze, then we can omit this\n",
    "        print('attention weights shape after expand_dims: ')\n",
    "        print(at.shape)\n",
    "        \n",
    "        # We calculate the weighted values -> \\alpha*hidden_states         \n",
    "        # row-wise multiplication (we are weighting the hidden_states, not the lstm_units) \n",
    "        output=x*at # (max_length x lstm_units)\n",
    "        print('weighted values shape: ')\n",
    "        print(output)\n",
    "        \n",
    "        # The output of this layer is the weighted values (we sum up the values of the hidden states), and\n",
    "        # the weights of the attetnion (max_length x 1)\n",
    "        return K.sum(output, axis=1), at\n",
    "    \n",
    "    # This is used for summary, to see the output shape of the two output matrices\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "    \n",
    "    # This is used for summary (it returns the params of the layer)\n",
    "    def get_config(self):\n",
    "        return super(BahdanauAttention, self).get_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape: \n",
      "(None, 32, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 32)\n",
      "attention weights shape: \n",
      "(None, 32)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 32, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"bahdanau_attention/mul:0\", shape=(None, 32, 10), dtype=float32)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 32, 16)            52144     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32, 10)            1080      \n",
      "_________________________________________________________________\n",
      "bahdanau_attention (Bahdanau ((None, 10), (None, 32, 1 42        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 53,277\n",
      "Trainable params: 53,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Attention, GlobalAveragePooling1D\n",
    "\n",
    "# Architecture\n",
    "inputs1 = Input(shape=(max_length,))\n",
    "embedding1 = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=max_length, embeddings_regularizer=l2(.001))\n",
    "embd_out1 = embedding1(inputs1)\n",
    "lstm1 = LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.2, return_sequences=True)\n",
    "lstm_out1 = lstm1(embd_out1)\n",
    "\n",
    "# attention = GlobalAveragePooling1D(Attention()([lstm_out1, lstm_out1]))\n",
    "weigthed_out, weights = BahdanauAttention()(lstm_out1)\n",
    "\n",
    "prob1 = Dense(1, activation='sigmoid')\n",
    "outputs1 = prob1(weigthed_out)\n",
    "\n",
    "model1 = Model(inputs1, outputs1) # classifier\n",
    "attention_model = Model(inputs1, weights) # attention weights\n",
    "\n",
    "\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "scores shape: \n",
      "(100, 32, 1)\n",
      "scores shape after squeeze: \n",
      "(100, 32)\n",
      "attention weights shape: \n",
      "(100, 32)\n",
      "attention weights shape after expand_dims: \n",
      "(100, 32, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model_1/bahdanau_attention/mul:0\", shape=(100, 32, 10), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/workshop/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape: \n",
      "(100, 32, 1)\n",
      "scores shape after squeeze: \n",
      "(100, 32)\n",
      "attention weights shape: \n",
      "(100, 32)\n",
      "attention weights shape after expand_dims: \n",
      "(100, 32, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model_1/bahdanau_attention/mul:0\", shape=(100, 32, 10), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/workshop/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1600 [===========================>..] - ETA: 0s - loss: 0.7231 - acc: 0.5000scores shape: \n",
      "(100, 32, 1)\n",
      "scores shape after squeeze: \n",
      "(100, 32)\n",
      "attention weights shape: \n",
      "(100, 32)\n",
      "attention weights shape after expand_dims: \n",
      "(100, 32, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model_1/bahdanau_attention/mul:0\", shape=(100, 32, 10), dtype=float32)\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 0.7224 - acc: 0.5006 - val_loss: 0.7112 - val_acc: 0.4125\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 1s 403us/sample - loss: 0.7012 - acc: 0.5225 - val_loss: 0.7041 - val_acc: 0.4125\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 1s 448us/sample - loss: 0.6928 - acc: 0.5219 - val_loss: 0.7028 - val_acc: 0.4125\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 1s 380us/sample - loss: 0.6890 - acc: 0.5219 - val_loss: 0.7024 - val_acc: 0.4125\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 1s 412us/sample - loss: 0.6850 - acc: 0.5263 - val_loss: 0.7017 - val_acc: 0.4275\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 1s 340us/sample - loss: 0.6691 - acc: 0.5981 - val_loss: 0.6789 - val_acc: 0.5350\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 1s 373us/sample - loss: 0.5987 - acc: 0.7469 - val_loss: 0.6059 - val_acc: 0.6925\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 1s 394us/sample - loss: 0.4722 - acc: 0.8400 - val_loss: 0.5696 - val_acc: 0.7575\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 1s 391us/sample - loss: 0.3771 - acc: 0.8988 - val_loss: 0.6057 - val_acc: 0.7450\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 1s 441us/sample - loss: 0.3173 - acc: 0.9200 - val_loss: 0.5685 - val_acc: 0.7625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd818934518>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model1.fit(x=x_train,y=y_train,\n",
    "          batch_size=100,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_test,y_test)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape: \n",
      "(None, 32, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 32)\n",
      "attention weights shape: \n",
      "(None, 32)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 32, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model_1/bahdanau_attention/mul:0\", shape=(None, 32, 10), dtype=float32)\n",
      "[   3 2866    5    2 1101   37   25   40   14 2867    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "['i miss it and wish they had one in philadelphia']\n",
      "(10, 32, 1)\n",
      "[[15]\n",
      " [14]\n",
      " [15]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [15]\n",
      " [15]\n",
      " [14]\n",
      " [14]]\n",
      "[[0.81437135]\n",
      " [0.23242   ]\n",
      " [0.9033072 ]\n",
      " [0.93976784]\n",
      " [0.22325104]\n",
      " [0.9174627 ]\n",
      " [0.8985529 ]\n",
      " [0.88971627]\n",
      " [0.26299414]\n",
      " [0.16395885]]\n"
     ]
    }
   ],
   "source": [
    "# This time the model faster\n",
    "# results change with each execusion \n",
    "# acc_train = 0.92 last epoch and acc_test = 0.76 last epoch\n",
    "\n",
    "pred = model1.predict(x_test[:10])\n",
    "# Attention is after embeddings and lstm, therefore this attention is to the abstract representation obtained\n",
    "# with the LSTM layer rather than to the words.\n",
    "attention_pred = attention_model.predict(x_test[:10]) \n",
    "\n",
    "print(x_test[0])\n",
    "print(t.sequences_to_texts(x_test[:1]))\n",
    "print(attention_pred.shape)\n",
    "print(np.argmax(attention_pred, axis=1))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
