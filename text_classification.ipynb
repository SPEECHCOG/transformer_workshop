{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with Transformer\n",
    "\n",
    "From [Keras API](https://keras.io/examples/nlp/text_classification_with_transformer/) by [Apoorv Nandan](https://twitter.com/NandanApoorv)\n",
    "\n",
    "In this example we will implement a transformer block (stacked encoders) for text classification. \n",
    "\n",
    "\n",
    "## Dataset IMDB\n",
    "\n",
    "The dataset corresponds to film reviews labelled with positive or negative sentiment (Andrew L. et al., 2011).\n",
    "\n",
    "### size\n",
    "\n",
    "train 25,000\n",
    "\n",
    "test 25,000\n",
    "\n",
    "unlabelled 50,000\n",
    "\n",
    "\n",
    "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/workshop/lib/python3.6/site-packages/tensorflow_core/python/keras/datasets/imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/andrea/miniconda3/envs/workshop/lib/python3.6/site-packages/tensorflow_core/python/keras/datasets/imdb.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     1   591   202    14    31     6   717    10    10 18142 10698     5\n",
      "     4   360     7     4   177  5760   394   354     4   123     9  1035\n",
      "  1035  1035    10    10    13    92   124    89   488  7944   100    28\n",
      "  1668    14    31    23    27  7479    29   220   468     8   124    14\n",
      "   286   170     8   157    46     5    27   239    16   179 15387    38\n",
      "    32    25  7944   451   202    14     6   717]\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 20000 # size of lexicon\n",
    "index_offset = 3\n",
    "max_len = 200\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=vocab_size, index_from=index_offset)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_val = pad_sequences(x_val, maxlen=max_len)\n",
    "\n",
    "# Example label and sequence\n",
    "print(y_val[0])\n",
    "print(x_val[0])\n",
    "\n",
    "# From (https://stackoverflow.com/a/44891281)\n",
    "word_to_idx = imdb.get_word_index()\n",
    "word_to_idx = {k: (v+index_offset) for k, v in word_to_idx.items()}\n",
    "# reserved tokens\n",
    "word_to_idx[\"<PAD>\"] = 0\n",
    "word_to_idx[\"<START>\"] = 1\n",
    "word_to_idx[\"<UNK>\"] = 2\n",
    "word_to_idx[\"<UNUSED>\"] = 3\n",
    "idx_to_word = {v:k for k,v in word_to_idx.items()}\n",
    "\n",
    "def from_seq_to_text(seq):\n",
    "    return ' '.join(idx_to_word[idx] for idx in seq)\n",
    "\n",
    "print(from_seq_to_text(x_val[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "![Architecture network](img/text_classifier_arch.png)\n",
    "\n",
    "### Parameters\n",
    "* Embedding size (Embed) = 32\n",
    "* Number of heads = 2\n",
    "* Feedforward network dimension = 32\n",
    "* Dropout = 10 %\n",
    "* First dense layer units (dim1) = 20\n",
    "* Last dense layer units (Classes) = 2\n",
    "\n",
    "### Embedding Block\n",
    "\n",
    "It consists of the word embeddings and positional embeddings. Keras provides an embedding layer.\n",
    "\n",
    "The embedding layer takes as input positive integers and turn them into dense vectors of embedding size. \n",
    "\n",
    "For word embeddings the integers are the indeces of the words in the lexicon, whereas in the positional embeddings they are the position in the sequence. \n",
    "\n",
    "In the embedding layer the model learns one matrix of size (input_dim x embedding_size) $W_{embed}$. Where input_dim is the vocabulary size for word embeddigns and maximum length of a sequence for the positional embeddings.\n",
    "\n",
    "The embedding block will be $W_{word} + W_{pos}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer block\n",
    "\n",
    "\n",
    "![Transformer block architecture](img/transformer_block_arch.png) \n",
    "![Transformer block architecture in the paper](img/transformer_block_paper.png)\n",
    "\n",
    "### Elements\n",
    "* Multi-head self-attention (more details below)\n",
    "\n",
    "* LayerNormalization, it normalises mean 0 and standar deviation of 1. The normalisation occurs across the last axis (or specified) at the sample level.\n",
    "\n",
    "* Feed forward network (FFN). An independend network (trained together) that should be applied in pararel to all the 'words' (attention output). We will use a Sequential model with two dense layers. The process goes as the first layer has a ReLU activation, and its output is feed to another dense layer.\n",
    "\n",
    "    * Equation: $FFN(x) = max(0,xW_1 + b1) W_2 + b_2$ \n",
    "    \n",
    "    Where $W_1$ has a size of (embed_dim x ffn_dim) and $W_2$ has a size of (ffn_dim x embed_dim). \n",
    "    \n",
    "    \n",
    "## Multi-head self-attention block\n",
    "![Multi-head self-attention architecture](img/multi_head_self_attention_arch.png)\n",
    "\n",
    "\n",
    "### Elements\n",
    "* Query, Key and Value matrices (all heads are concatenated). Dense layers\n",
    "    * Input (BatchxSeqxEmbed)\n",
    "    * Output (BatchxSeqx(Heads*d_model)) In this example d_model is calculated automatically as `embed_dim // num_heads`\n",
    "* Separate heads. It reorganises the query, key and value tensors so that we can calculate the attention per head using the batch and heads dimensions as batch dimensions. \n",
    "    * Input (BatchxSeqx(Heads*d_model))\n",
    "    * Output (BatchxHeadsxSeqxd_model)\n",
    "    * Process: reshape input to (BatchxSeqxHeadsxd_model) then permute Seq and Heads dimensions, and reshape to (BatchxHeadsxSeqxd_model)\n",
    "* Self attention:\n",
    "    * Equation: $\\text{Attention}(Q,KV) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "* Concatenat heads. Again, this is used to reorganise the matrices so we have all heads concatenated as in Query, Key and Value matrices. \n",
    "    * Input (BatchxHeadsxSeqxd_model)\n",
    "    * Output (BatchxSeqxEmbed)\n",
    "    * Process: permutate Heads and Seq dimensions, and then reshape to (BatchxSeqxEmbed)\n",
    "* Multi-head matrix. The output of all heads should be combined into one tensor of the shape as the embedding tensors, because that is what the feed forward network is expecting, one tensor for word. A dense layer.\n",
    "    * Equation: $\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O$\n",
    "    \n",
    "    where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "    * The size of $W^O$ is ((Heads*d_model)xd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head self-attention\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value, mask):\n",
    "        score = tf.matmul(query, key, transpose_b=True) # (batch_size, )\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # d_model\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        if mask is not None:\n",
    "            # look ahead mask\n",
    "            scaled_score += (mask * -1e9) # -inf for masked values         \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value, mask)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LayerNormalization, Dropout\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
    "        ) # FeedForward Network\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        # if training=True the layer add dropout, in inference mode it does nothing.\n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) # Residual connection (+)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "Now let's create the classifier model\n",
    "\n",
    "![Classifier architecture model](img/text_classifier_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- dimensions -----------------------\n",
      "Shape embedding layer: \n",
      "(None, 200, 32)\n",
      "Shape Transformer block: \n",
      "(None, 200, 32)\n",
      "Shape global average pooling layer: \n",
      "(None, 32)\n",
      "Shape dropout layer: \n",
      "(None, 32)\n",
      "Shape dense 1 layer: \n",
      "(None, 20)\n",
      "Shape dropout layer: \n",
      "(None, 20)\n",
      "Shape dense 2 (output) layer: \n",
      "(None, 2)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 200, 32)           646400    \n",
      "_________________________________________________________________\n",
      "transformer_block_2 (Transfo (None, 200, 32)           6464      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 653,566\n",
      "Trainable params: 653,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "num_classes = 2\n",
    "dense_dim1 = 20 # Reducing dimensions after attention\n",
    "\n",
    "inputs = Input(shape=(max_len,))\n",
    "embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "print('----------------------- dimensions -----------------------')\n",
    "print('Shape embedding layer: ')\n",
    "print(x.shape)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "print('Shape Transformer block: ')\n",
    "print(x.shape)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "print('Shape global average pooling layer: ')\n",
    "print(x.shape)\n",
    "x = Dropout(0.1)(x)\n",
    "print('Shape dropout layer: ')\n",
    "print(x.shape)\n",
    "x = Dense(dense_dim1, activation=\"relu\")(x)\n",
    "print('Shape dense 1 layer: ')\n",
    "print(x.shape)\n",
    "x = Dropout(0.1)(x)\n",
    "print('Shape dropout layer: ')\n",
    "print(x.shape)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "print('Shape dense 2 (output) layer: ')\n",
    "print(outputs.shape)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 64s 3ms/sample - loss: 0.3922 - accuracy: 0.8140 - val_loss: 0.3023 - val_accuracy: 0.8688\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 64s 3ms/sample - loss: 0.2006 - accuracy: 0.9232 - val_loss: 0.3056 - val_accuracy: 0.8751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff83469eb70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss\n",
      "[0.8410611  0.15893884]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "print(from_seq_to_text(x_val[0]))\n",
    "print(predictions[0])\n",
    "print(y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
